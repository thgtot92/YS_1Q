import google.generativeai as genai
import os
import time
from google.api_core.exceptions import ResourceExhausted

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    print("âš ï¸ groq íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("LLaMA ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´: pip install groq")
    GROQ_AVAILABLE = False
    Groq = None

from typing import Literal

# API í‚¤ ì„¤ì •
genai.configure(api_key=os.getenv("GOOGLE_API_KEY") or "AIzaSyC-2aYPlx7he6pwCGmhvYYenvYGluEZApA")

# Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (LLaMAìš©)
GROQ_API_KEY = os.getenv("GROQ_API_KEY") or "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"

if GROQ_AVAILABLE:
    try:
        groq_client = Groq(api_key=GROQ_API_KEY)
        print(f"âœ… Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ (í‚¤: {GROQ_API_KEY[:20]}...)")
    except Exception as e:
        print(f"âŒ Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        groq_client = None
        GROQ_AVAILABLE = False
else:
    groq_client = None

def get_llm_report(prompt: str, max_retries=2, retry_delay=60, model_type: Literal["gemini", "llama"] = "gemini"):
    """
    Rate Limitì„ ê³ ë ¤í•œ ì•ˆì „í•œ LLM ë¦¬í¬íŠ¸ ìƒì„± (Gemini/LLaMA ì„ íƒ ê°€ëŠ¥)
    
    Args:
        prompt: LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ 2íšŒ)
        retry_delay: ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„(ì´ˆ, ê¸°ë³¸ 60ì´ˆ)
        model_type: ì‚¬ìš©í•  ëª¨ë¸ ("gemini" ë˜ëŠ” "llama")
    
    Returns:
        str: LLM ì‘ë‹µ ë˜ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€
    """
    if model_type == "llama":
        return get_llama_report(prompt, max_retries, retry_delay)
    else:
        return get_gemini_report(prompt, max_retries, retry_delay)

def reinitialize_groq_client(api_key=None):
    """Groq í´ë¼ì´ì–¸íŠ¸ë¥¼ ì¬ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜"""
    global groq_client, GROQ_API_KEY, GROQ_AVAILABLE
    
    if api_key:
        GROQ_API_KEY = api_key
        os.environ["GROQ_API_KEY"] = api_key
    else:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°
        GROQ_API_KEY = os.getenv("GROQ_API_KEY", GROQ_API_KEY)
    
    if not GROQ_AVAILABLE:
        try:
            from groq import Groq
            GROQ_AVAILABLE = True
        except ImportError:
            print("âš ï¸ Groq íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
    
    try:
        groq_client = Groq(api_key=GROQ_API_KEY)
        print(f"âœ… Groq í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì„±ê³µ (í‚¤: {GROQ_API_KEY[:20]}...)")
        return True
    except Exception as e:
        print(f"âŒ Groq í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        groq_client = None
        return False

def get_gemini_report(prompt: str, max_retries=2, retry_delay=60):
    """ê¸°ì¡´ Gemini ëª¨ë¸ ì‚¬ìš©"""
    model = genai.GenerativeModel("models/gemini-1.5-flash")
    
    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                print(f"â³ API ìš”ì²­ í•œë„ë¡œ ì¸í•´ {retry_delay}ì´ˆ ëŒ€ê¸° ì¤‘... (ì¬ì‹œë„ {attempt}/{max_retries})")
                time.sleep(retry_delay)
            
            print(f"ğŸ§  Gemini LLM ë¶„ì„ ì§„í–‰ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries + 1})")
            response = model.generate_content(prompt)
            
            if response and response.text:
                print("âœ… Gemini ë¶„ì„ ì™„ë£Œ")
                return response.text
            else:
                print("âš ï¸ Gemini ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
                return "[ë¶„ì„ ì‹¤íŒ¨] Gemini ì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except ResourceExhausted as e:
            print(f"âš ï¸ Gemini API ìš”ì²­ í•œë„ ì´ˆê³¼ (ì‹œë„ {attempt + 1})")
            print("ğŸ’¡ Gemini Free TierëŠ” ë¶„ë‹¹ 15íšŒ ìš”ì²­ ì œí•œì´ ìˆìŠµë‹ˆë‹¤.")
            
            if attempt >= max_retries:
                return create_fallback_analysis(prompt)
                
        except Exception as e:
            print(f"âŒ Gemini í˜¸ì¶œ ì˜¤ë¥˜: {str(e)[:100]}...")
            
            if attempt >= max_retries:
                return create_fallback_analysis(prompt)
            else:
                print(f"ğŸ”„ 10ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(10)
    
    return create_fallback_analysis(prompt)

def get_llama_report(prompt: str, max_retries=2, retry_delay=30, debug=False):
    """
    LLaMA ëª¨ë¸ì„ ì‚¬ìš©í•œ ë¦¬í¬íŠ¸ ìƒì„± (Groq API ì‚¬ìš©)
    
    Args:
        prompt: LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        retry_delay: ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)
        debug: ë””ë²„ê·¸ ëª¨ë“œ (ìƒì„¸ ì •ë³´ ì¶œë ¥)
    
    Returns:
        str: LLaMA ì‘ë‹µ ë˜ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€
    """
    global groq_client, GROQ_API_KEY
    
    # Groq ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if not GROQ_AVAILABLE:
        print("âŒ Groq íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("í•´ê²° ë°©ë²•: pip install groq")
        return create_fallback_analysis(prompt)
    
    # groq_clientê°€ Noneì´ë©´ ì¬ì´ˆê¸°í™” ì‹œë„
    if not groq_client:
        print("âš ï¸ Groq í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¬ì´ˆê¸°í™” ì‹œë„ ì¤‘...")
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë‹¤ì‹œ ì½ê¸°
        new_key = os.getenv("GROQ_API_KEY", GROQ_API_KEY)
        if reinitialize_groq_client(new_key):
            print("âœ… Groq í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì„±ê³µ")
        else:
            print("âŒ Groq í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨")
            return create_fallback_analysis(prompt)
    
    if debug:
        print(f"ğŸ” Debug: Groq API Key = {GROQ_API_KEY[:20]}...")
        print(f"ğŸ” Debug: Groq Client = {groq_client}")
        print(f"ğŸ” Debug: Environment GROQ_API_KEY = {os.getenv('GROQ_API_KEY', 'Not set')[:20]}...")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ LLaMA ëª¨ë¸ë“¤
    models = [
        "llama3-8b-8192",           # ê°€ì¥ ì•ˆì •ì ì¸ ëª¨ë¸
        "llama-3.1-8b-instant",     # ë¹ ë¥¸ ì‘ë‹µ
        "llama3-70b-8192",          # ëŒ€í˜• ëª¨ë¸
        "llama-3.1-70b-versatile"   # ìµœì‹  ëŒ€í˜• ëª¨ë¸
    ]
    
    selected_model = models[0]  # ê¸°ë³¸ì ìœ¼ë¡œ ê°€ì¥ ì•ˆì •ì ì¸ ëª¨ë¸ ì‚¬ìš©
    
    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                print(f"â³ ì¬ì‹œë„ ëŒ€ê¸° ì¤‘... {retry_delay}ì´ˆ (ì‹œë„ {attempt}/{max_retries})")
                time.sleep(retry_delay)
                # ì¬ì‹œë„ ì‹œ ë” ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ì „í™˜
                if attempt < len(models):
                    selected_model = models[attempt]
                    print(f"ğŸ”„ ëª¨ë¸ ë³€ê²½: {selected_model}")
            
            print(f"ğŸ¦™ LLaMA ({selected_model}) ë¶„ì„ ì§„í–‰ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries + 1})")
            
            if debug:
                print(f"ğŸ” Debug: ëª¨ë¸ = {selected_model}")
                print(f"ğŸ” Debug: í”„ë¡¬í”„íŠ¸ ê¸¸ì´ = {len(prompt)} ë¬¸ì")
            
            # Groq API í˜¸ì¶œ
            chat_completion = groq_client.chat.completions.create(
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ í•œêµ­ ì£¼ì‹ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                model=selected_model,
                temperature=0.7,
                max_tokens=2048,
                top_p=0.9,
                stream=False
            )
            
            response_text = chat_completion.choices[0].message.content
            
            if response_text:
                print(f"âœ… LLaMA ({selected_model}) ë¶„ì„ ì™„ë£Œ")
                return response_text
            else:
                print("âš ï¸ LLaMA ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
                return "[ë¶„ì„ ì‹¤íŒ¨] LLaMA ì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ LLaMA í˜¸ì¶œ ì˜¤ë¥˜: {error_msg[:200]}...")
            
            # ìì„¸í•œ ì—ëŸ¬ ì§„ë‹¨
            if "rate_limit_exceeded" in error_msg.lower():
                print("âš ï¸ Groq API ìš”ì²­ í•œë„ ì´ˆê³¼")
                print("- ë¬´ë£Œ í”Œëœ: ë¶„ë‹¹ 30 ìš”ì²­, ì¼ì¼ 14,400 í† í°")
                if attempt < max_retries:
                    print(f"ğŸ”„ {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    continue
            
            # API í‚¤ ì˜¤ë¥˜
            elif "authentication" in error_msg.lower() or "api_key" in error_msg.lower() or "401" in error_msg:
                print("âŒ Groq API í‚¤ ì¸ì¦ ì˜¤ë¥˜!")
                print(f"í˜„ì¬ ì„¤ì •ëœ í‚¤: {GROQ_API_KEY[:20]}...")
                print("\ní•´ê²° ë°©ë²•:")
                print("1. ì œê³µëœ API í‚¤ê°€ ë§Œë£Œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                print("2. https://console.groq.com ì—ì„œ ìƒˆ ê³„ì • ìƒì„±")
                print("3. API Keys ë©”ë‰´ì—ì„œ ìƒˆ í‚¤ ë°œê¸‰")
                print("4. ì½”ë“œì—ì„œ ì§ì ‘ ìˆ˜ì •í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ ì„¤ì •:")
                print("   export GROQ_API_KEY='your-new-key'")
                return create_fallback_analysis(prompt)
            
            # ëª¨ë¸ ì˜¤ë¥˜
            elif "model" in error_msg.lower() or "not found" in error_msg.lower():
                print(f"âŒ ëª¨ë¸ ì˜¤ë¥˜: {selected_model}")
                print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: llama3-8b-8192, llama-3.1-8b-instant")
                if attempt < max_retries and attempt < len(models):
                    selected_model = models[attempt + 1]
                    print(f"ğŸ”„ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì¬ì‹œë„: {selected_model}")
                    continue
            
            # ê¸°íƒ€ ì˜¤ë¥˜
            else:
                print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ")
                print("ì „ì²´ ì˜¤ë¥˜ ë©”ì‹œì§€:")
                print(error_msg)
            
            if attempt >= max_retries:
                return create_fallback_analysis(prompt)
    
    return create_fallback_analysis(prompt)

def compare_llm_models(prompt: str):
    """
    Geminiì™€ LLaMA ëª¨ë¸ì˜ ì‘ë‹µì„ ë¹„êµ
    
    Args:
        prompt: ë‘ ëª¨ë¸ì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
    
    Returns:
        dict: ë‘ ëª¨ë¸ì˜ ì‘ë‹µê³¼ ë©”íƒ€ë°ì´í„°
    """
    print("ğŸ”¬ LLM ëª¨ë¸ ë¹„êµ ë¶„ì„ ì‹œì‘")
    print("="*50)
    
    # Gemini ë¶„ì„
    print("\n1ï¸âƒ£ Gemini ë¶„ì„")
    start_time = time.time()
    gemini_response = get_llm_report(prompt, model_type="gemini", max_retries=1)
    gemini_time = time.time() - start_time
    
    # ì ì‹œ ëŒ€ê¸° (API ì œí•œ ë°©ì§€)
    print("\nâ³ ëª¨ë¸ ì „í™˜ ëŒ€ê¸° (5ì´ˆ)...")
    time.sleep(5)
    
    # LLaMA ë¶„ì„
    print("\n2ï¸âƒ£ LLaMA ë¶„ì„")
    start_time = time.time()
    llama_response = get_llm_report(prompt, model_type="llama", max_retries=1)
    llama_time = time.time() - start_time
    
    print("\nâœ… ëª¨ë¸ ë¹„êµ ì™„ë£Œ")
    print(f"â±ï¸ Gemini ì‘ë‹µ ì‹œê°„: {gemini_time:.2f}ì´ˆ")
    print(f"â±ï¸ LLaMA ì‘ë‹µ ì‹œê°„: {llama_time:.2f}ì´ˆ")
    
    return {
        "gemini": {
            "response": gemini_response,
            "time": gemini_time,
            "model": "gemini-1.5-flash"
        },
        "llama": {
            "response": llama_response,
            "time": llama_time,
            "model": "llama-3.1-70b-versatile"
        }
    }

def create_fallback_analysis(prompt: str) -> str:
    """
    LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë¶„ì„ ìƒì„±
    í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê¸°ë³¸ì ì¸ ë¶„ì„ ì œê³µ
    """
    
    # í”„ë¡¬í”„íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
    lines = prompt.split('\n')
    
    # ì¢…ëª©ëª…, ë‚ ì§œ, ì´ë²¤íŠ¸ ì •ë³´ ì¶”ì¶œ
    stock_info = ""
    event_info = ""
    news_info = ""
    disclosure_info = ""
    
    for line in lines:
        if "ì´ë²¤íŠ¸:" in line:
            event_info = line.strip()
        elif "ë‚ ì§œ:" in line or "ì‹œê°„:" in line:
            stock_info += line.strip() + " "
        elif "ë‰´ìŠ¤" in line and ":" in line:
            news_info = "ë‰´ìŠ¤ ì •ë³´ í¬í•¨"
        elif "ê³µì‹œ" in line and ":" in line:
            disclosure_info = "ê³µì‹œ ì •ë³´ í¬í•¨"
    
    fallback_analysis = f"""[ëŒ€ì²´ ë¶„ì„ ë¦¬í¬íŠ¸]

{stock_info.strip()}

ğŸ“Š **ë¶„ì„ ìƒí™©:**
- LLM API ìš”ì²­ í•œë„ ì´ˆê³¼ ë˜ëŠ” ì˜¤ë¥˜ë¡œ ì¸í•´ ëŒ€ì²´ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
- ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ë³¸ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

ğŸ“ˆ **ì£¼ìš” ë‚´ìš©:**
{event_info if event_info else "â€¢ ì£¼ê°€ ë³€ë™ ì´ë²¤íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."}

ğŸ“° **ë‰´ìŠ¤ ì˜í–¥:**
â€¢ {news_info if news_info else "ê´€ë ¨ ë‰´ìŠ¤ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤."}
â€¢ ë‰´ìŠ¤ ë‚´ìš©ê³¼ ì£¼ê°€ ë³€ë™ ê°„ì˜ ì—°ê´€ì„±ì„ í™•ì¸í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

ğŸ“‹ **ê³µì‹œ ì˜í–¥:**
â€¢ {disclosure_info if disclosure_info else "ê´€ë ¨ ê³µì‹œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤."}
â€¢ ê³µì‹œ ë‚´ìš©ì´ ì£¼ê°€ì— ë¯¸ì¹œ ì˜í–¥ì„ ë¶„ì„í•´ë³¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

ğŸ” **ì¢…í•© í‰ê°€:**
â€¢ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”í›„ ìƒì„¸ ë¶„ì„ì´ ê¶Œì¥ë©ë‹ˆë‹¤.
â€¢ API í•œë„ ë³µêµ¬ í›„ ì¬ë¶„ì„ì„ í†µí•´ ë” ì •í™•í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---
*ëŒ€ì²´ ë¶„ì„ ì‹œìŠ¤í…œì— ì˜í•´ ìƒì„±ëœ ë¦¬í¬íŠ¸ì…ë‹ˆë‹¤.*
*ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.*
"""
    
    return fallback_analysis


def analyze_events_with_llm(events_df, matched_news_dict, stock_name, model_type="gemini"):
    """
    ì´ë²¤íŠ¸ë³„ ë¶„ì„ - Rate Limit ê³ ë ¤ ë²„ì „
    """
    summaries = []
    
    # ì´ë²¤íŠ¸ê°€ ë§ì„ ê²½ìš° ì£¼ìš” ì´ë²¤íŠ¸ë§Œ ë¶„ì„ (API í˜¸ì¶œ ìµœì†Œí™”)
    if len(events_df) > 3:
        print(f"âš ï¸ ì´ë²¤íŠ¸ê°€ {len(events_df)}ê°œë¡œ ë§ì•„ ìƒìœ„ 3ê°œë§Œ ë¶„ì„í•©ë‹ˆë‹¤. (API í•œë„ ê³ ë ¤)")
        events_df = events_df.head(3)
    
    print(f"ğŸ“Š ì´ {len(events_df)}ê°œ ì´ë²¤íŠ¸ ë¶„ì„ ì‹œì‘ (ëª¨ë¸: {model_type})")
    
    for i, row in events_df.iterrows():
        time_obj = row["datetime"]
        price = row["price"]
        event_type = row["event_type"]
        news_list = matched_news_dict.get(time_obj, [])
        news_summary_text = "\n".join([f"- {n['title']}" for n in news_list]) if news_list else "- ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ"

        prompt = f"""[ì´ë²¤íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ ë³´ê³ ì„œ]

ğŸ“… ë‚ ì§œ: {time_obj.strftime('%Y-%m-%d')}
ğŸ•’ ì‹œê°„: {time_obj.strftime('%H:%M')}
ğŸ“ˆ ì´ë²¤íŠ¸: {stock_name}ì˜ ì£¼ê°€ê°€ {event_type} (ê°€ê²©: {price}ì›)

ğŸ“° ë‹¹ì‹œ ë‰´ìŠ¤ ëª©ë¡:
{news_summary_text}

ì´ ë‰´ìŠ¤ì™€ ì£¼ê°€ ë³€í™”ì˜ ê´€ë ¨ì„±ì„ ë¶„ì„í•˜ê³ , ê°€ëŠ¥í•œ ì›ì¸ê³¼ ì‹œì‚¬ì ì„ í¬í•¨í•˜ì—¬ 3~5ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
"""
        
        # API í˜¸ì¶œ ê°„ê²© ì¡°ì • (Rate Limit ë°©ì§€)
        if i > 0:
            print("â³ API ìš”ì²­ ê°„ê²© ì¡°ì • ì¤‘...")
            time.sleep(5)  # 5ì´ˆ ëŒ€ê¸°
            
        result = get_llm_report(prompt, max_retries=1, retry_delay=30, model_type=model_type)
        summaries.append((time_obj.strftime('%H:%M'), event_type, result))
        print(f"âœ… ì´ë²¤íŠ¸ {i+1} ë¶„ì„ ì™„ë£Œ")
    
    return summaries


def create_comprehensive_analysis(events_df, matched_news_dict, matched_disclosures_dict, stock_name, date, model_type="gemini"):
    """
    ì¢…í•© ë¶„ì„ ìƒì„± - ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ LLM í˜¸ì¶œë¡œ ì²˜ë¦¬ (íš¨ìœ¨ì )
    """
    
    # ì´ë²¤íŠ¸ ìš”ì•½
    event_summary = ""
    if len(events_df) > 0:
        event_summary = "ğŸ“ˆ ì£¼ìš” ì´ë²¤íŠ¸:\n"
        for _, event in events_df.iterrows():
            pct = event['pct_from_start'] * 100
            event_summary += f"- {event['datetime'].strftime('%H:%M')}: {pct:.2f}% {event['event_type']} (â‚©{event['price']:,})\n"
    else:
        event_summary = "ğŸ“ˆ ì£¼ìš” ì´ë²¤íŠ¸:\n- 1% ì´ìƒì˜ ì£¼ìš” ë³€ë™ ì´ë²¤íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
    
    # ë‰´ìŠ¤ ìš”ì•½
    all_news = []
    for news_list in matched_news_dict.values():
        all_news.extend(news_list)
    
    # ì¤‘ë³µ ì œê±°
    unique_news = []
    seen_titles = set()
    for news in all_news:
        if news['title'] not in seen_titles:
            seen_titles.add(news['title'])
            unique_news.append(news)
    
    news_summary = ""
    if unique_news:
        news_summary = "ğŸ“° ê´€ë ¨ ë‰´ìŠ¤:\n"
        for news in unique_news[:6]:  # ìƒìœ„ 6ê°œ
            news_summary += f"- {news['title']}\n"
    else:
        news_summary = "ğŸ“° ê´€ë ¨ ë‰´ìŠ¤:\n- í•´ë‹¹ ì‹œì ê³¼ ì—°ê´€ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    
    # ê³µì‹œ ìš”ì•½
    all_disclosures = []
    for disc_list in matched_disclosures_dict.values():
        all_disclosures.extend(disc_list)
    
    # ì¤‘ë³µ ì œê±°
    unique_disclosures = []
    seen_disc_titles = set()
    for disc in all_disclosures:
        if disc['title'] not in seen_disc_titles:
            seen_disc_titles.add(disc['title'])
            unique_disclosures.append(disc)
    
    disclosure_summary = ""
    if unique_disclosures:
        disclosure_summary = "ğŸ“‹ ê´€ë ¨ ê³µì‹œ:\n"
        for disc in unique_disclosures[:4]:  # ìƒìœ„ 4ê°œ
            disclosure_summary += f"- {disc['title']}\n"
    else:
        disclosure_summary = "ğŸ“‹ ê´€ë ¨ ê³µì‹œ:\n- í•´ë‹¹ ê¸°ê°„ ê´€ë ¨ ê³µì‹œê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    
    # í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„±
    comprehensive_prompt = f"""[{date} {stock_name} CRAG ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸]

{event_summary}

{news_summary}

{disclosure_summary}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **ì£¼ê°€ ë™í–¥ í•µì‹¬**: ë‹¹ì¼ ì£¼ìš” ê°€ê²© ë³€ë™ê³¼ ê±°ë˜ íŠ¹ì§•
2. **ë‰´ìŠ¤ ì˜í–¥ ë¶„ì„**: ë‰´ìŠ¤ê°€ ì£¼ê°€ì— ë¯¸ì¹œ ì˜í–¥ê³¼ ì‹œì¥ ë°˜ì‘  
3. **ê³µì‹œ ì˜í–¥ ë¶„ì„**: ê³µì‹œì •ë³´ê°€ ì£¼ê°€ì— ë¯¸ì¹œ ì˜í–¥ê³¼ íˆ¬ìì ë°˜ì‘
4. **ì¸ê³¼ê´€ê³„ ì¢…í•©**: ë‰´ìŠ¤ì™€ ê³µì‹œ ì¤‘ ì£¼ìš” ë³€ë™ ìš”ì¸ ë¶„ì„
5. **í–¥í›„ ì „ë§**: ì˜¤ëŠ˜ì˜ íë¦„ì´ í–¥í›„ ì£¼ê°€ì— ë¯¸ì¹  ì˜í–¥

ì „ë¬¸ì ì´ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    
    print(f"ğŸ§  CRAG ì¢…í•© ë¶„ì„ ì§„í–‰ ì¤‘... (ëª¨ë¸: {model_type})")
    return get_llm_report(comprehensive_prompt, max_retries=2, retry_delay=60, model_type=model_type)

def test_groq_connection():
    """Groq API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Groq API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    if not GROQ_AVAILABLE:
        print("âŒ Groq íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("ì„¤ì¹˜: pip install groq")
        return False
    
    if not groq_client:
        print("âŒ Groq í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        return False
    
    print(f"API í‚¤: {GROQ_API_KEY[:20]}...")
    
    try:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
        response = groq_client.chat.completions.create(
            messages=[
                {"role": "user", "content": "Say 'Hello, I'm working!' in Korean"}
            ],
            model="llama3-8b-8192",  # ê°€ì¥ ì•ˆì •ì ì¸ ëª¨ë¸ ì‚¬ìš©
            max_tokens=50,
            temperature=0.5
        )
        
        result = response.choices[0].message.content
        print(f"âœ… Groq API ì—°ê²° ì„±ê³µ!")
        print(f"ì‘ë‹µ: {result}")
        return True
        
    except Exception as e:
        print(f"âŒ Groq API ì—°ê²° ì‹¤íŒ¨!")
        print(f"ì˜¤ë¥˜: {str(e)}")
        
        error_str = str(e)
        if "authentication" in error_str.lower() or "401" in error_str:
            print("\nğŸ”§ ì¸ì¦ ì˜¤ë¥˜ - í•´ê²° ë°©ë²•:")
            print("1. API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”")
            print("2. https://console.groq.com/keys ì—ì„œ í‚¤ ìƒíƒœ í™•ì¸")
            print("3. ìƒˆ í‚¤ ë°œê¸‰ í›„ ë‹¤ì‹œ ì‹œë„")
            print(f"4. í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ í‚¤: {GROQ_API_KEY}")
            print("   ì´ í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif "rate" in error_str.lower():
            print("\nâš ï¸ Rate Limit ì´ˆê³¼")
            print("- ë¬´ë£Œ í‹°ì–´: ë¶„ë‹¹ 30 ìš”ì²­ ì œí•œ")
            print("- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”")
        elif "model" in error_str.lower():
            print("\nğŸ”§ ëª¨ë¸ ì˜¤ë¥˜")
            print("ë‹¤ë¥¸ ëª¨ë¸ì„ ì‹œë„í•´ë³´ì„¸ìš”:")
            print("- llama3-8b-8192")
            print("- llama-3.1-8b-instant")
        
        return False


# # í…ŒìŠ¤íŠ¸ ì½”ë“œ
# if __name__ == "__main__":
#     print("="*50)
#     print("LLM Reporter í…ŒìŠ¤íŠ¸")
#     print("="*50)
    
#     # Groq ìƒíƒœ í™•ì¸
#     if not GROQ_AVAILABLE:
#         print("\nâš ï¸ Groq/LLaMAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#         print("Geminië§Œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
#         print("\nLLaMAë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:")
#         print("1. pip install groq")
#         print("2. ìŠ¤í¬ë¦½íŠ¸ ë‹¤ì‹œ ì‹¤í–‰")
#     else:
#         print(f"\nâœ… Groq API í‚¤ ì„¤ì •ë¨: {GROQ_API_KEY[:20]}...")
        
#         # API ì—°ê²° í…ŒìŠ¤íŠ¸
#         print("\n" + "="*50)
#         if test_groq_connection():
#             print("\nâœ… API ì—°ê²° í…ŒìŠ¤íŠ¸ í†µê³¼! ëª¨ë¸ ë¹„êµë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n")
#         else:
#             print("\nâŒ Groq API ì—°ê²° ì‹¤íŒ¨")
#             print("ìƒˆ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤:")
#             print("1. https://console.groq.com ì ‘ì†")
#             print("2. ë¬´ë£Œ ê³„ì • ìƒì„± (Google/GitHub ë¡œê·¸ì¸ ê°€ëŠ¥)")
#             print("3. API Keys ë©”ë‰´ì—ì„œ 'Create API Key' í´ë¦­")
#             print("4. ìƒì„±ëœ í‚¤ë¥¼ ë³µì‚¬")
#             print("5. ì½”ë“œì˜ GROQ_API_KEY ë¶€ë¶„ì„ ìƒˆ í‚¤ë¡œ êµì²´")
#             print("\në˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì •:")
#             print("export GROQ_API_KEY='your-new-key'")
    
#     # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
#     print("\n" + "="*50)
#     test_prompt = "ì‚¼ì„±ì „ì ì£¼ê°€ê°€ 2% ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤. ì´ì— ëŒ€í•œ ê°„ë‹¨í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."
    
#     if GROQ_AVAILABLE and groq_client:
#         print("ğŸ§ª ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸")
#         comparison = compare_llm_models(test_prompt)
        
#         print("\nğŸ“Š Gemini ì‘ë‹µ:")
#         print(comparison["gemini"]["response"][:200] + "...")
        
#         print("\nğŸ¦™ LLaMA ì‘ë‹µ:")
#         print(comparison["llama"]["response"][:200] + "...")
#     else:
#         print("ğŸ§ª Gemini ë‹¨ë… í…ŒìŠ¤íŠ¸")
#         response = get_gemini_report(test_prompt)
#         print("\nğŸ“Š Gemini ì‘ë‹µ:")
#         print(response[:200] + "...")